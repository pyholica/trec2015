{
 "metadata": {
  "name": "",
  "signature": "sha256:e564df6e4dbac99048e39b310de7a7d8497d8c528082ab907d3d89c9ab093783"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from elasticsearch import Elasticsearch\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "#import MongoEx\n",
      "\n",
      "class ElasticTraining:\n",
      "    def __init__(self):\n",
      "        self.es = Elasticsearch([{'host':'localhost','port':9200}])\n",
      "        self.que = pd.read_csv(open('query2014.csv'),sep='\\t')\n",
      "        self.ans = pd.read_csv(open('answer2014.csv'),sep='\\t')\n",
      "        self.field = ['title','body','abstract']\n",
      "        self.scheme = ['tfidf', 'bm25','ib','lmd','lmj','dfr']\n",
      "\n",
      "    def buildPairDB(self):\n",
      "        filename1 = 'pair_answer2014.csv'\n",
      "        filename2 = 'eval_answer2014.csv'\n",
      "        \n",
      "        tByTopic = []\n",
      "\n",
      "        ans = pd.read_csv(open('answer2014.csv'),sep='\\t')\n",
      "        for i in range(1,31):\n",
      "            tByTopic.append(ans[ans['topic'] == i])\n",
      "\n",
      "        \n",
      "        l = pd.DataFrame()\n",
      "        eva = pd.DataFrame()\n",
      "\n",
      "        cum =  0\n",
      "        for topic_bundle in tByTopic:\n",
      "            relnum = len(topic_bundle[(topic_bundle['relevancy']==2) | (topic_bundle['relevancy']==1)])\n",
      "            cnt = (relnum*4)/5\n",
      "            zerocnt = cnt\n",
      "            cum = cum + cnt\n",
      "            print \"COUNT:\",cnt\n",
      "            print \"CUM :\", cum\n",
      "            for idx,entry in topic_bundle.iterrows():\n",
      "                if ((entry['relevancy'] == 1) or (entry['relevancy'] == 2)) and (not cnt == 0):\n",
      "                    l = l.append(pd.DataFrame({\n",
      "                                \"pmcid\" : [entry['pmcid']],\n",
      "                                \"topic\" : [entry['topic']],\n",
      "                                \"relevancy\" : [entry['relevancy']]\n",
      "                                }))\n",
      "                    cnt = cnt - 1\n",
      "                elif (entry['relevancy'] == 0) and (not zerocnt == 0):\n",
      "                    l = l.append(pd.DataFrame({\n",
      "                                \"pmcid\" : [entry['pmcid']],\n",
      "                                \"topic\" : [entry['topic']],\n",
      "                                \"relevancy\" : [entry['relevancy']]\n",
      "                                }))\n",
      "                    zerocnt = zerocnt - 1\n",
      "                else:\n",
      "                    eva = eva.append(pd.DataFrame({\n",
      "                                \"pmcid\" : [entry['pmcid']],\n",
      "                                \"topic\" : [entry['topic']],\n",
      "                                \"relevancy\" : [entry['relevancy']]\n",
      "                                }))\n",
      "\n",
      "        l.to_csv(filename1,sep='\\t',index=False)\n",
      "        eva.to_csv(filename2,sep='\\t',index=False)\n",
      "\n",
      "    def search_scheme(self,scheme,num,ds):\n",
      "        filename = \"search_result/\"+ scheme + \"_\"+ds+\"_\"+str(num)+\".csv\"\n",
      "        pmcList = []\n",
      "        relevancyList = []\n",
      "\n",
      "        for index,entry in self.que.iterrows():\n",
      "            if entry['topic'] == num:\n",
      "                query = entry\n",
      "                break\n",
      "\n",
      "        for index,entry in self.ans.iterrows():\n",
      "            if entry['topic'] == num:\n",
      "                pmcList.append(entry['pmcid'])\n",
      "                relevancyList.append(entry['relevancy'])\n",
      "                \n",
      "        reTable = pd.DataFrame({\"pmcid\" : pmcList, \"relevancy\" : relevancyList})\n",
      "\n",
      "        content = query[ds].replace(r\"/\",',')\n",
      "        analyzer = \"my_\"+scheme+\"_analyzer\"\n",
      "\n",
      "        res = self.es.search(index=scheme +\"_garam\",q=content,doc_type='article',analyzer=analyzer,size=40000,request_timeout=120)\n",
      "\n",
      "        l = pd.DataFrame()\n",
      "        for entry in res['hits']['hits']:\n",
      "            if entry['_source']['topicnum'] == num:\n",
      "                pmcid = entry['_source']['pmcid']\n",
      "                score = entry['_score']\n",
      "                l = l.append(pd.DataFrame({\"pmcid\" : [pmcid],\"score\" : [score]}))\n",
      "\n",
      "        l = pd.merge(l,reTable,how='inner',on=['pmcid'])\n",
      "        l = l.fillna(0)\n",
      "        l.to_csv(filename,sep='\\t',index=False)\n",
      "\n",
      "    def search_field(self,num,ds,scheme):\n",
      "        filename = \"search_result/\"+\"field_\" + scheme + \"_\" +ds + \"_\" + str(num) + \".csv\"\n",
      "        for index,entry in self.que.iterrows():\n",
      "            if entry['topic'] == num:\n",
      "                query = entry\n",
      "                break\n",
      "        pmcList = []\n",
      "        relevancyList = []\n",
      "\n",
      "        for index,entry in self.ans.iterrows():\n",
      "            if entry['topic'] == num:\n",
      "                pmcList.append(entry['pmcid'])\n",
      "                relevancyList.append(entry['relevancy'])\n",
      "\n",
      "        reTable = pd.DataFrame({\"pmcid\":pmcList,\"relevancy\":relevancyList})\n",
      "\n",
      "        content = query[ds].replace(r\"/\",',')\n",
      "        token = content.split(' ')\n",
      "        content = [ x for idx,x in enumerate(token) if not idx == 0]\n",
      "        content = ' '.join(content)\n",
      "\n",
      "        analyzer = \"my_\"+scheme+\"_analyzer\"\n",
      "\n",
      "        resTitle= self.es.search(index=scheme+\"_garam\",q='title:' + content,doc_type='article',analyzer=analyzer,size=40000,request_timeout=200)\n",
      "        print \"Done with title :\",len(resTitle['hits']['hits'])\n",
      "        l = pd.DataFrame()\n",
      "        for entry in resTitle['hits']['hits']:\n",
      "            if entry['_source']['topicnum'] == num:\n",
      "                pmcid = entry['_source']['pmcid']\n",
      "                score = entry['_score']\n",
      "                l = l.append(pd.DataFrame({\"pmcid\":[pmcid], 'title':[score]}))\n",
      "\n",
      "        resAbstract= self.es.search(index=scheme+\"_garam\",q='abstract:'+content,doc_type='article',analyzer=analyzer,size=40000,request_timeout=200)\n",
      "        print \"Done with abstract :\",len(resAbstract['hits']['hits'])\n",
      "        resBody= self.es.search(index=scheme+\"_garam\",q='body:'+content,doc_type='article',analyzer=analyzer,size=40000,request_timeout=200)\n",
      "        print \"Done with body :\",len(resBody['hits']['hits'])\n",
      "        \n",
      "\n",
      "        v = l\n",
      "        l = pd.DataFrame()\n",
      "        \n",
      "        for entry in resAbstract['hits']['hits']:\n",
      "            if entry['_source']['topicnum'] == num:\n",
      "                pmcid = entry['_source']['pmcid']\n",
      "                score = entry['_score']\n",
      "                l = l.append(pd.DataFrame({\"pmcid\":[pmcid], 'abstract' : [score]}))\n",
      "        print \"V:\",len(v)\n",
      "        print \"L:\",len(l)\n",
      "    \n",
      "        v = pd.merge(v,l,how='outer',on=['pmcid'])\n",
      "        l = pd.DataFrame()\n",
      "        for entry in resBody['hits']['hits']:\n",
      "            if entry['_source']['topicnum'] == num:\n",
      "                pmcid = entry['_source']['pmcid']\n",
      "                score = entry['_score']\n",
      "                l = l.append(pd.DataFrame({\"pmcid\":[pmcid], 'body' : [score]}))\n",
      "\n",
      "        v = pd.merge(v,l,how='outer',on=['pmcid'])\n",
      "        v=v.fillna(0)\n",
      "        v = pd.merge(v,reTable,how='inner',on=['pmcid'])\n",
      "        v=v.fillna(0)\n",
      "        v.to_csv(filename,sep='\\t',index=False)\n",
      "\n",
      "    def training_scheme3(self,s1,s2,s3,ds):\n",
      "        l = pd.DataFrame()\n",
      "        for i in range(1,31):\n",
      "            print \"working on topic\",str(i)\n",
      "            filename1 = 'scheme_' + s1 + '_' + ds + '_' + str(i) + '_training.csv'\n",
      "            filename2 = 'scheme_' + s2 + '_' + ds + '_' + str(i) + '_training.csv'\n",
      "            filename3 = 'scheme_' + s3 + '_' + ds + '_' + str(i) + '_training.csv'\n",
      "\n",
      "            data1 = pd.read_csv(open(\"vector/\"+filename1),sep='\\t')\n",
      "            data2 = pd.read_csv(open(\"vector/\"+filename2),sep='\\t')\n",
      "            data3 = pd.read_csv(open(\"vector/\"+filename3),sep='\\t')\n",
      "\n",
      "            data1 = data1.rename(columns={'score':s1})\n",
      "            data2 = data2.rename(columns={'score':s2})\n",
      "            data3 = data3.rename(columns={'score':s3})\n",
      "\n",
      "            m = pd.merge(data1,data2,how='outer',on=['pmcid','relevancy'])\n",
      "            m = pd.merge(m,data3,how='outer',on=['pmcid','relevancy'])\n",
      "            m = m.fillna(0)\n",
      "\n",
      "            em_min = float('inf')\n",
      "            remember_alpha = 0\n",
      "            remember_beta = 0\n",
      "\n",
      "            for alpha in np.arange(0,1,0.01):\n",
      "                for beta in np.arange(0,1,0.01):\n",
      "                    normA = m[s1]/m[s1].sum()\n",
      "                    normB = m[s2]/m[s2].sum()\n",
      "                    normC = m[s3]/m[s3].sum()\n",
      "\n",
      "                    score = (1-alpha)*(1-beta)*normA + (1-alpha)*beta*normB + alpha*normC\n",
      "                    relevancy = m['relevancy']\n",
      "\n",
      "                    em = (relevancy-score) ** 2\n",
      "\n",
      "                    if em.sum() < em_min:\n",
      "                        em_min = em.sum()\n",
      "                        remember_alpha = alpha\n",
      "                        remember_beta = beta\n",
      "            l = l.append(pd.DataFrame({\n",
      "                        'scheme1' : [s1],\n",
      "                        'scheme2' : [s2],\n",
      "                        'scheme3' : [s3],\n",
      "                        'topic' : [i],\n",
      "                        'loss' : [em_min],\n",
      "                        'alpha' : [(1-remember_alpha)*(1-remember_beta)],\n",
      "                        'beta' : [(1-remember_alpha)*remember_beta],\n",
      "                        'gamma' : [remember_alpha]\n",
      "                        }))\n",
      "        \n",
      "        l.to_csv('analysis/' + 'scheme_' + s1 +'_' +s2 + '_' + s3 + '_' + ds + '.csv',sep='\\t',index=False)     \n",
      "\n",
      "    def training_scheme(self,s1,s2,ds):\n",
      "        l = pd.DataFrame()\n",
      "        for i in range(1,31):\n",
      "            filename1 = 'scheme_' + s1 + '_' + ds + '_' + str(i) + '_training.csv'\n",
      "            filename2 = 'scheme_' + s2 + '_' + ds + '_' + str(i) + '_training.csv'\n",
      "\n",
      "            data1 = pd.read_csv(open(\"vector/\"+filename1),sep='\\t')\n",
      "            data2 = pd.read_csv(open(\"vector/\"+filename2),sep='\\t')\n",
      "\n",
      "            data1 = data1.rename(columns={'score':s1})\n",
      "            data2 = data2.rename(columns={'score':s2})\n",
      "\n",
      "            m = pd.merge(data1,data2,how='outer',on=['pmcid','relevancy'])\n",
      "            m = m.fillna(0)\n",
      "            \n",
      "            min_em = float(\"inf\")\n",
      "            remember_alpha = 0\n",
      "\n",
      "            for alpha in np.arange(0,1,0.01):\n",
      "                normA = m[s1]/m[s1].sum()\n",
      "                normB = m[s2]/m[s2].sum()\n",
      "                \n",
      "                score= alpha*normA + (1-alpha)*normB\n",
      "                relevancy = m['relevancy']\n",
      "                \n",
      "                em = (relevancy - score) ** 2\n",
      "                \n",
      "                if em.sum() < min_em:\n",
      "                    min_em = em.sum()\n",
      "                    remember_alpha = alpha\n",
      "                    \n",
      "            l = l.append(pd.DataFrame( \n",
      "                    {\n",
      "                        'scheme1' : [s1], \n",
      "                        'scheme2' : [s2], \n",
      "                        'ds' : [ds], \n",
      "                        'topic' : [i], \n",
      "                        'loss'  : [min_em], \n",
      "                        'alpha' : [remember_alpha],\n",
      "                        'beta' : [1-remember_alpha]\n",
      "                        }\n",
      "                    ))\n",
      "        l.to_csv('analysis/' + s1+'_'+s2+'_'+ds+'.csv',sep='\\t',index=False)\n",
      "        \n",
      "    def test(self):\n",
      "         # Find the topic we are dealing with\n",
      "        for entry in self.que:\n",
      "            if entry['number'] == str(1):\n",
      "                query = entry\n",
      "                break\n",
      "\n",
      "        content = query['description'].replace(r\"/\",\",\")\n",
      "        res = self.es.search(index='tfidf',q=content,doc_type=\"article\",analyzer=\"my_tfidf_analyzer\",size=1500)\n",
      "\n",
      "        print str(res['hits']['hits'][0]['_score'])\n",
      "                 \n",
      "        \n",
      "    def buildVectorWithScheme(self,num,scheme,ds='summary'):\n",
      "        filename = scheme + \"_\" + ds + \"_\" + str(num) + \".csv\"\n",
      "        filename_training = 'scheme_' + scheme + '_' + ds + '_' + str(num) + '_training.csv'\n",
      "        filename_eval = 'scheme_' + scheme + '_' + ds + '_' + str(num) + '_eval.csv'\n",
      "        print \"Working on\",filename\n",
      "        data = pd.read_csv(open('search_result/' + filename),sep='\\t')\n",
      "        data_shuffle = data.iloc[np.random.permutation(len(data))]\n",
      "\n",
      "        training = pd.DataFrame()\n",
      "        evaluation = pd.DataFrame()\n",
      "\n",
      "        cnt = len(data[(data['relevancy'] == 1) | (data['relevancy'] == 2)])*4/5\n",
      "        zero_cnt = cnt\n",
      "\n",
      "        for index,entry in data_shuffle.iterrows():\n",
      "            if entry['relevancy'] == 0:\n",
      "                if zero_cnt == 0:\n",
      "                    evaluation = evaluation.append(entry)\n",
      "                else:\n",
      "                    # training = training.append(entry)\n",
      "                    zero_cnt = zero_cnt -1\n",
      "            else:\n",
      "                if cnt==0:\n",
      "                    evaluation = evaluation.append(entry)\n",
      "                else:\n",
      "                    training = training.append(entry)\n",
      "                    cnt = cnt - 1\n",
      "\n",
      "        training.to_csv('vector/'+filename_training,sep='\\t',index=False)\n",
      "        evaluation.to_csv('vector/'+filename_eval,sep='\\t',index=False)\n",
      "        \n",
      "            \n",
      "    def buildVectorWithField(self,scheme,num,ds='summary'):\n",
      "        filename = \"field\" + \"_\" + scheme+\"_\" + ds + \"_\"  + str(num) + \".csv\"\n",
      "        filename_training = \"field\" + \"_\" + scheme+\"_\" + ds + \"_\"  + str(num) + \"_training.csv\"\n",
      "        filename_eval = \"field\" + \"_\" + scheme+ \"_\" + ds + \"_\" + str(num) + \"_eval.csv\" \n",
      "        print \"Working on\",filename\n",
      "        data = pd.read_csv(open(\"search_result/\"+filename),sep='\\t')\n",
      "        data_shuffle = data.iloc[np.random.permutation(len(data))]\n",
      "\n",
      "        training = pd.DataFrame()\n",
      "        evaluation = pd.DataFrame()\n",
      "\n",
      "        cnt = len(data[(data['relevancy'] == 1) | (data['relevancy'] == 2)])*4/5\n",
      "        zero_cnt = cnt\n",
      "        \n",
      "\n",
      "        for idx,entry in data_shuffle.iterrows():\n",
      "            if entry['relevancy'] == 0:\n",
      "                if zero_cnt == 0:\n",
      "                    evaluation = evaluation.append(entry)\n",
      "                else:\n",
      "                    #training = training.append(entry)\n",
      "                    zero_cnt = zero_cnt - 1\n",
      "            else:\n",
      "                if cnt == 0:\n",
      "                    evaluation = evaluation.append(entry)\n",
      "                else:\n",
      "                    training = training.append(entry)\n",
      "                    cnt = cnt - 1\n",
      "\n",
      "        training.to_csv('vector/'+filename_training,sep='\\t',index=False)\n",
      "        evaluation.to_csv('vector/'+filename_eval,sep='\\t',index=False)  \n",
      "\n",
      "      \n",
      "\n",
      "    def training_field(self,scheme,ds):\n",
      "\n",
      "        l = pd.DataFrame()\n",
      "        for i in range(1,31):\n",
      "            print \"Topic :\",str(i)\n",
      "            em_min = float(\"inf\")\n",
      "            remember_alpha = 0\n",
      "            remember_beta = 0\n",
      "            filename = 'field_' + scheme + '_' + ds + '_' + str(i) + '_training.csv'\n",
      "            data = pd.read_csv(open(\"vector/\"+filename),sep='\\t')\n",
      "            data.drop_duplicates(subset='pmcid',take_last=True,inplace=True)\n",
      "            \n",
      "            for alpha in np.arange(0,1,0.01):\n",
      "                for beta in np.arange(0,1,0.01):\n",
      "                    normA = data['title']/data['title'].sum()\n",
      "                    normB = data['abstract']/data['abstract'].sum()\n",
      "                    normC = data['body']/data['body'].sum()\n",
      "\n",
      "                    score = (1-alpha)*(1-beta)*normA + (1-alpha)*beta*normB + alpha*normC\n",
      "                    relevancy = data['relevancy']\n",
      "\n",
      "                    em = (relevancy - score) ** 2\n",
      "\n",
      "                    if em.sum() < em_min:\n",
      "                        em_min = em.sum()\n",
      "                        remember_alaph = alpha\n",
      "                        remember_beta = beta\n",
      "            print \"Alpha:\",(1-remember_alpha)*(1-remember_beta)\n",
      "            print \"Beta:\",(1-remember_alpha)*remember_beta\n",
      "            print \"Gamma:\",remember_alpha\n",
      "\n",
      "            l = l.append(pd.DataFrame({\n",
      "                'scheme' : [scheme],\n",
      "                'ds' : [ds],\n",
      "                'topic' : [i],\n",
      "                'loss' : [em_min],\n",
      "                'alpha' : [(1-remember_alpha)*(1-remember_beta)],\n",
      "                'beta' : [(1-remember_alpha)*remember_beta],\n",
      "                'gamma' : [remember_alpha]\n",
      "            }))\n",
      "        l.to_csv('analysis/' +'field_' + scheme + '_' + ds+ '.csv',sep='\\t',index=False)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "et = ElasticTraining()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "et.training_field()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}